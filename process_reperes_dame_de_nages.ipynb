{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "process_reperes_dame_de_nages",
      "provenance": [],
      "authorship_tag": "ABX9TyMt3U33Q49RvzkJmUbW7EME",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/evaie/evaie/blob/main/process_reperes_dame_de_nages.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKcTDp9NsHWx"
      },
      "source": [
        "#Get references data\n",
        "Find Mattieu's file and read it into a dataframe\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K4rsDYrFr8GR"
      },
      "source": [
        "# piece of code that get a token\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# authenticate the user\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHlhhC2IwO6K"
      },
      "source": [
        "Once authenticated, list the files that contains personal records for the atheletes\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmM_u1sywXSy",
        "outputId": "03fc9ecd-f933-4911-faa8-0a59b56e8aa4"
      },
      "source": [
        "# Auto-iterate through all files that matches this query\n",
        "#file_list = drive.ListFile({'q': \"'root' in parents and title contains 'Champs' and trashed=false\"}).GetList()\n",
        "#sharedWithMe=true \n",
        "# id = 1VJMXVNSxIv4li8yK7pXZ3sitWEdC8z-93z_Apkl77_Q\n",
        "file_list = drive.ListFile({'q': \"sharedWithMe=true and trashed=false and title contains '2021 - repères d'\"}).GetList()\n",
        "for file1 in file_list:\n",
        "  print('title: %s, id: %s' % (file1['title'], file1['id']))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "title: 2021 - repères d'intensité, id: 1VJMXVNSxIv4li8yK7pXZ3sitWEdC8z-93z_Apkl77_Q\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAvSnNJI_BCJ"
      },
      "source": [
        "once the id of the spreadsheet retrieved, load it. \n",
        "But first, we need to be allowed Gspread  / GSheet to access to our data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHdv2xxc_KEd"
      },
      "source": [
        "# piece of code that get a token\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "import gspread\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "gc = gspread.authorize(GoogleCredentials.get_application_default())"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxxEFhZpOfzB"
      },
      "source": [
        "Then load, the data into a dataframe.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6o1K8HxjQ_68"
      },
      "source": [
        "sh = gc.open_by_key('1VJMXVNSxIv4li8yK7pXZ3sitWEdC8z-93z_Apkl77_Q')\n",
        "worksheet = sh.get_worksheet(0)\n",
        "# get_all_values gives a list of rows.\n",
        "rows = worksheet.get_all_values()\n",
        "#print(rows)\n",
        "\n",
        "import pandas as pd\n",
        "df_reperes = pd.DataFrame.from_records(rows)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NMEvXueWRlb"
      },
      "source": [
        "Reprocess the dataframe : renaming the columns\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7WGqBH9GJEIJ"
      },
      "source": [
        "# column names un 1st row\n",
        "# reformat\n",
        "headers = df_reperes.iloc[0].apply(lambda x: str(x).replace(' ', '_'))\n",
        "headers = headers.apply(lambda x: str(x).replace('(', ''))\n",
        "headers = headers.apply(lambda x: str(x).replace(')', ''))\n",
        "headers = headers.apply(lambda x: str(x).replace('.', ''))\n",
        "headers = headers.apply(lambda x: str(x).replace('é', 'e'))\n",
        "headers = headers.apply(lambda x: str(x).replace('_/_', '_'))\n",
        "headers = headers.apply(lambda x: str(x).replace(',', '_'))\n",
        "headers = headers.apply(lambda x: str(x).lower())\n",
        "\n",
        "# rename columns\n",
        "df_reperes = df_reperes.rename(columns=headers)\n",
        "df_reperes = df_reperes.drop(0)\n",
        "\n",
        "# add rower column\n",
        "df_reperes['rower'] = df_reperes['prenom'] + ' ' + df_reperes['nom']\n",
        "df_reperes['rower'] = df_reperes['rower'].apply(lambda x: str(x).replace('é', 'e'))\n",
        "df_reperes['rower'] = df_reperes['rower'].apply(lambda x: str(x).replace('è', 'e'))\n",
        "df_reperes['rower'] = df_reperes['rower'].apply(lambda x: str(x).lower())\n"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0Jeqyi8eGCi"
      },
      "source": [
        "# Get the data from the outriggers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ddL5otne9Wq",
        "outputId": "5a53b3c5-e20c-4dce-92be-bdb9c2807691"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "# files whose title contains SpdCoach and not in the trash\n",
        "file_list = drive.ListFile({'q': \"title contains 'SpdCoach' and trashed=false\"}).GetList()\n",
        "ddd_list = list()\n",
        "for ddd_file in file_list:\n",
        "  # collect meta data from the filename\n",
        "  filename = ddd_file['title']\n",
        "  file_id = ddd_file['id']\n",
        "  tokens = filename.split('-')\n",
        "  rower = tokens[1].strip().lower()\n",
        "  training_type = tokens[2].split('.')[0].strip()\n",
        "  date = tokens[0].split(' ')[2].strip() +':'+ tokens[0].split(' ')[3].strip()\n",
        "  print('rower: %s, training type: %s, date: %s' % (rower, training_type, date))\n",
        "  \n",
        "  # this assume pyDrive has a token to read the GoogleDrive\n",
        "  myfile = drive.CreateFile({'id': file_id})\n",
        "  myfile.GetContentFile('file.csv')\n",
        "  \n",
        "  # then read it in a Dataframe\n",
        "  ddd = pd.read_csv('file.csv', skiprows=27, header=None)\n",
        "\n",
        "  # clean and organize the data\n",
        "  # column names un 1st row\n",
        "  # reformat\n",
        "  headers = ddd.iloc[0].apply(lambda x: str(x).replace(' ', '_'))\n",
        "  headers = headers.apply(lambda x: str(x).replace('(', ''))\n",
        "  headers = headers.apply(lambda x: str(x).replace(')', ''))\n",
        "  headers = headers.apply(lambda x: str(x).replace('.', ''))\n",
        "  headers = headers.apply(lambda x: str(x).lower())\n",
        "\n",
        "  # rename columns\n",
        "  ddd = ddd.rename(columns=headers)\n",
        "\n",
        "  # dropping NA values\n",
        "  ddd2 = ddd.dropna()\n",
        "\n",
        "  # drop the 2 first lines\n",
        "  ddd2 = ddd2.drop([0, 1])\n",
        "\n",
        "  # drop the 2 last rows\n",
        "  #ddd2 = ddd2.drop(ddd2.tail(2).index)\n",
        "\n",
        "  # reset the index\n",
        "  ddd2 = ddd2.reset_index(drop=True)\n",
        "\n",
        "  # changing column types\n",
        "  column_names = list(ddd2.columns)\n",
        "\n",
        "  # datetime columns\n",
        "  # remove lines with date time columns formated to '---'\n",
        "  ddd2 = ddd2.drop(ddd2[ddd2['elapsed_time'].map(lambda x: str(x) == '---')].index)\n",
        "  ddd2 = ddd2.drop(ddd2[ddd2['split_gps'].map(lambda x: str(x) == '---')].index)\n",
        "  ddd2 = ddd2.drop(ddd2[ddd2['split_imp'].map(lambda x: str(x) == '---')].index)\n",
        "\n",
        "  # iterating on columns and change the types to string\n",
        "  for col in column_names:\n",
        "      ddd2[col] = ddd2[col].apply(lambda x: str(x).replace('---', '0'))\n",
        "\n",
        "  ddd2 = ddd2.convert_dtypes()\n",
        "\n",
        "  # removing datetime columns from the columns list\n",
        "  column_names.pop(3)  # 'elapsed_time'\n",
        "  column_names.pop(3)  # 'split_gps'\n",
        "  column_names.pop(4)  # 'split_imp'\n",
        "\n",
        "  # add meta data\n",
        "  ddd2['rower'] = rower\n",
        "  ddd2['training_type'] = training_type\n",
        "  ddd2['date'] = date\n",
        "\n",
        "  for col in column_names:\n",
        "    ddd2[col] = pd.to_numeric(ddd2[col])\n",
        "\n",
        "  # accumulate ddd\n",
        "  ddd_list.append(ddd2)\n"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "rower: louis marchal, training type: 1000, date: 20210331:0420pm\n",
            "rower: louis marchal, training type: recup, date: 20210403:0824am\n",
            "rower: louis marchal, training type: 500, date: 20210403:0814am\n",
            "rower: adrien lecomte, training type: 500 et recup, date: 20210403:0814am\n",
            "rower: louis marchal, training type: 2000 finale, date: 20210404:1259pm\n",
            "rower: yohan bara butez, training type: 500, date: 20210403:0814am\n",
            "rower: yohan bara butez, training type: 2000 finale, date: 20210404:1236pm\n",
            "rower: adrien lecomte, training type: 2000 série, date: 20210404:1007am\n",
            "rower: yohan bara butez, training type: 2000 série, date: 20210404:0936am\n",
            "rower: adrien lecomte, training type: 1000, date: 20210331:0420pm\n",
            "rower: yohan bara butez, training type: 1000, date: 20210331:0253pm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwVnTWAxcmSU"
      },
      "source": [
        "# concat dataframes\n",
        "all_ddd = pd.concat(ddd_list)\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLZ9ptL3YHy1"
      },
      "source": [
        "Save the reperes worksheet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VepKo4jmp7OI"
      },
      "source": [
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "\n",
        "# this piece of code assumes we already have a token to read / write a Google Sheet\n",
        "# 1e_ThufABmmmbUlqQ05VoCrXfChhDuQHMnZQMYw73NxA is the id of DameDeNage_database\n",
        "db_sh = gc.open_by_key('1e_ThufABmmmbUlqQ05VoCrXfChhDuQHMnZQMYw73NxA')\n",
        "\n",
        "wsheet = db_sh.worksheet('reperes')\n",
        "gd.set_with_dataframe(wsheet,df_reperes)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p31JzST2GTg4"
      },
      "source": [
        "Adding a workseet with the dame de nage data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aX8Q04tMH7yO"
      },
      "source": [
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "\n",
        "# this piece of code assumes we already have a token to read / write a Google Sheet\n",
        "# 1e_ThufABmmmbUlqQ05VoCrXfChhDuQHMnZQMYw73NxA is the id of DameDeNage_database\n",
        "db_sh = gc.open_by_key('1e_ThufABmmmbUlqQ05VoCrXfChhDuQHMnZQMYw73NxA')\n",
        "\n",
        "\n",
        "worksheet_data = db_sh.worksheet('data')\n",
        "gd.set_with_dataframe(worksheet_data,all_ddd)"
      ],
      "execution_count": 30,
      "outputs": []
    }
  ]
}